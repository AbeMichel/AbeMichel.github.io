<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>ASL Recognition - Abraham Michel</title>

    <link rel="stylesheet" href="../StyleSheets/main.css"> 
    <link rel="stylesheet" href="../StyleSheets/project.css"> 
    <link rel="stylesheet" href="../StyleSheets/navbar.css"> 
</head>
<body>
    <nav class="navbar">
        <button id="nav-menu-btn">&#9776;</button>
        <div id="nav-links">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../experience.html">Work Experience</a></li>
                <li><a href="../projects.html">Projects</a></li>
                <li><a href="../aboutme.html">About Me</a></li>
                <li><a href="https://www.linkedin.com/in/abrahammichel">LinkedIn</a></li>
                <li><a href="https://github.com/AbeMichel">GitHub</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main>
        <section id="project-header">
            <div>
                <h1>ASL Recognition</h1>
            </div>
        </section>
        <section id="content">
            <h1>Summary of Lessons</h1>
            <ol>
                <li><p>Split big complex problems into small digestable ones</p></li>
            </ol>
            <h1>The full version</h1>
            <p>
                This might be one of my favorite projects to date. Since I was little I've been facinated with non-traditional languages
                and the expressive nature of American Sign Language (ASL) so it seemed like a fantastic challenge to building a machine Learning
                model trained on more dynamic signs. I had messed around with machine learning in the past, building a 
                <a href="FacialRecognition.html">facial recognition from scratch</a>, but that was with static images while the goal for this was
                to recognize dynamic movements. I opted to use libraries for this one which made it a lot simpler, but one of the biggest improvements I made
                since my last attempt at machine learning was code structure and documentation. A lot of that was from my time at <a href="../experience.html#jd">John Deere</a>.
                <br/>
                <br/>
                There were a couple problems I faced with this project: where do I find enough data and how do I store such a large amount of data.
                <br/>
                <br/>
                The first was partially addressed by choosing to work with GIFs, as they're basically muted videos, and decreasing the frame rate I recorded at.
                I treated each frame in the GIF as
                a series of images and ran them through a hand recognition library 
                (<a href="https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker">mediapipe</a>). This process filtered out the noise of the images that commonly
                causes issues in machine learning and let my model focus on the important parts: hand position and shape. I learned that it's easier to 
                <b class="lesson" id="l0">split big complex problems into small digestable ones</b>.

            </p>
            
        </section>
        <section>
            <h1>Photo Credits</h1>
            <ol>
            </ol>
        </section>
    </main>
    <!-------------->

    <!-- Scripts -->
    <script src="../Scripts/navbarMenuToggle.js"></script>
    <!-------------->
</body>
</html>